{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bank Customer Churn Prediction\n",
    "\n",
    "This notebook implements a machine learning pipeline to predict customer churn in banking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('BankChurners.csv')\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic data information\n",
    "print(\"\\nDataset Info:\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations directory\n",
    "if not os.path.exists('visualizations'):\n",
    "    os.makedirs('visualizations')\n",
    "\n",
    "# Plot distribution of target variable\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df, x='Attrition_Flag')\n",
    "plt.title('Distribution of Customer Attrition')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/attrition_distribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a few key numeric features\n",
    "num_features = ['Customer_Age', 'Credit_Limit', 'Total_Revolving_Bal', 'Total_Trans_Amt', 'Avg_Utilization_Ratio']\n",
    "\n",
    "# Plot histograms\n",
    "df[num_features].hist(bins=20, figsize=(15, 10), color='lightblue', edgecolor='black')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots to compare distributions by churn status\n",
    "for feature in num_features:\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    sns.boxplot(data=df, x='Attrition_Flag', y=feature)\n",
    "    plt.title(f'{feature} by Churn Status')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['Gender', 'Education_Level', 'Marital_Status', 'Income_Category', 'Card_Category']\n",
    "\n",
    "for feature in cat_features:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.countplot(data=df, x=feature, hue='Attrition_Flag')\n",
    "    plt.title(f'{feature} vs Churn')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode Attrition_Flag for correlation analysis\n",
    "df_corr = df.copy()\n",
    "df_corr['Attrition_Flag'] = df_corr['Attrition_Flag'].map({'Existing Customer': 0, 'Attrited Customer': 1})\n",
    "\n",
    "# Select numeric columns\n",
    "corr = df_corr.select_dtypes(include=['number']).corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(corr, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select small subset to avoid long processing\n",
    "sampled_df = df.sample(500, random_state=42)\n",
    "\n",
    "# Pairplot for selected features\n",
    "sns.pairplot(sampled_df, hue='Attrition_Flag', vars=['Customer_Age', 'Credit_Limit', 'Total_Trans_Amt', 'Avg_Utilization_Ratio'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze numeric variables\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "plt.figure(figsize=(15, 10))\n",
    "df[numeric_cols].hist(bins=30, figsize=(15, 10))\n",
    "# plt.tight_layout()\n",
    "plt.savefig('visualizations/numeric_distributions.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle categorical variables\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "label_encoders = {}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if col != 'Attrition_Flag':\n",
    "        le = LabelEncoder()\n",
    "        df[col] = le.fit_transform(df[col])\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Create target variable\n",
    "df['target'] = (df['Attrition_Flag'] == 'Attrited Customer').astype(int)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Attrition_Flag', 'CLIENTNUM']\n",
    "df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create derived features\n",
    "df['Credit_Utilization_Ratio'] = df['Total_Trans_Amt'] / df['Credit_Limit']\n",
    "df['Average_Transaction'] = df['Total_Trans_Amt'] / df['Total_Trans_Ct']\n",
    "\n",
    "# Handle infinite values\n",
    "df = df.replace([np.inf, -np.inf], np.nan)\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(\"New features created:\")\n",
    "print(df[['Credit_Utilization_Ratio', 'Average_Transaction']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Drop Naive Bayes columns if they exist\n",
    "df = df.drop([\n",
    "    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1',\n",
    "    'Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2'\n",
    "], axis=1, errors='ignore')\n",
    "\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build the Neural Network\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation='sigmoid')  # Binary classification output\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=30,\n",
    "    batch_size=32,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict on test data\n",
    "y_pred_prob = model.predict(X_test_scaled).ravel()\n",
    "y_pred = (y_pred_prob >= 0.5).astype(int)\n",
    "\n",
    "# Evaluation\n",
    "print(\"âœ… Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nðŸ“‹ Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=['Stayed', 'Churned'], yticklabels=['Stayed', 'Churned'])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.title('Confusion Matrix - Neural Network')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model performance\n",
    "print(\"Model Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('visualizations/confusion_matrix.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot accuracy\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance plot\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(10))\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.tight_layout()\n",
    "plt.savefig('visualizations/feature_importance.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = df.copy()\n",
    "label_encoders = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credit utilization ratio\n",
    "processed_data['Credit_Utilization_Ratio'] = processed_data['Total_Trans_Amt'] / processed_data['Credit_Limit']\n",
    "processed_data['Average_Transaction'] = processed_data['Total_Trans_Amt'] / processed_data['Total_Trans_Ct']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['Contact_Rate'] = processed_data['Contacts_Count_12_mon'] / 12\n",
    "processed_data['Inactive_Ratio'] = processed_data['Months_Inactive_12_mon'] / 12\n",
    "processed_data['Inactivity_Risk'] = pd.cut(\n",
    "    processed_data['Months_Inactive_12_mon'], \n",
    "    bins=[0, 1, 3, 6, 12], \n",
    "    labels=['Low', 'Medium', 'High', 'Very High']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['Value_Segment'] = processed_data['Total_Relationship_Count'] * processed_data['Income_Category']\n",
    "processed_data['Spending_Behavior_Change'] = processed_data['Total_Amt_Chng_Q4_Q1'] - processed_data['Total_Ct_Chng_Q4_Q1']\n",
    "processed_data['Unusual_Spending'] = ((processed_data['Total_Amt_Chng_Q4_Q1'] > 2) | \n",
    "                                      (processed_data['Total_Amt_Chng_Q4_Q1'] < 0.5)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['Age_Group'] = pd.cut(\n",
    "    processed_data['Customer_Age'], \n",
    "    bins=[0, 30, 40, 50, 60, 100], \n",
    "    labels=['Young', 'Thirties', 'Forties', 'Fifties', 'Senior']\n",
    ")\n",
    "\n",
    "processed_data['Revolving_Balance_Ratio'] = processed_data['Total_Revolving_Bal'] / processed_data['Credit_Limit']\n",
    "processed_data['Zero_Balance_Flag'] = (processed_data['Total_Revolving_Bal'] == 0).astype(int)\n",
    "processed_data['Dependents_Per_Age'] = processed_data['Dependent_count'] / processed_data['Customer_Age']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['Education_Income_Interaction'] = processed_data['Education_Level'] * processed_data['Income_Category']\n",
    "processed_data = processed_data.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Fill missing numeric values with median\n",
    "numeric_cols = processed_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numeric_cols:\n",
    "    processed_data[col] = processed_data[col].fillna(processed_data[col].median())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = processed_data.select_dtypes(include=['object', 'category']).columns\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    processed_data[col] = le.fit_transform(processed_data[col])\n",
    "    label_encoders[col] = le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, PolynomialFeatures\n",
    "\n",
    "\n",
    "numerical_features = ['Customer_Age', 'Total_Relationship_Count', 'Months_on_book']\n",
    "poly_features = processed_data[numerical_features].copy()\n",
    "\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True)\n",
    "poly_transformed = poly.fit_transform(poly_features)\n",
    "feature_names = poly.get_feature_names_out(numerical_features)\n",
    "poly_df = pd.DataFrame(poly_transformed, columns=feature_names)\n",
    "\n",
    "interaction_features = [col for col in poly_df.columns if ' ' in col]\n",
    "processed_data = pd.concat([processed_data, poly_df[interaction_features]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['Recency_Score'] = 12 - processed_data['Months_Inactive_12_mon']\n",
    "processed_data['Frequency_Score'] = processed_data['Total_Trans_Ct']\n",
    "processed_data['Monetary_Score'] = processed_data['Total_Trans_Amt']\n",
    "\n",
    "processed_data['RFM_Score'] = (\n",
    "    processed_data['Recency_Score'] / processed_data['Recency_Score'].max() +\n",
    "    processed_data['Frequency_Score'] / processed_data['Frequency_Score'].max() +\n",
    "    processed_data['Monetary_Score'] / processed_data['Monetary_Score'].max()\n",
    ") / 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['Total_Trans_Amt', 'Credit_Limit', 'Avg_Open_To_Buy']:\n",
    "    if col in processed_data.columns:\n",
    "        processed_data[f'{col}_Log'] = np.log1p(processed_data[col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data['Tenure_Ratio'] = processed_data['Months_on_book'] / (processed_data['Customer_Age'] * 12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "cluster_features = ['Customer_Age', 'Total_Relationship_Count', 'Total_Trans_Amt', 'Total_Trans_Ct']\n",
    "cluster_data = processed_data[cluster_features].copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "cluster_data_scaled = scaler.fit_transform(cluster_data)\n",
    "\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "processed_data['Customer_Segment'] = kmeans.fit_predict(cluster_data_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get weights from the input layer\n",
    "input_weights = model.layers[0].get_weights()[0]  # Shape: (num_features, num_neurons)\n",
    "feature_importance = np.mean(np.abs(input_weights), axis=1)  # Aggregate across neurons\n",
    "\n",
    "# Plot\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(range(len(sorted_idx)), X.columns[sorted_idx])\n",
    "plt.xlabel(\"Average Absolute Weight\")\n",
    "plt.title(\"Input Layer Weight-Based Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
